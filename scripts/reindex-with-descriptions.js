import { Pinecone } from '@pinecone-database/pinecone';
import OpenAI from 'openai';
import fs from 'fs/promises';
import path from 'path';
import dotenv from 'dotenv';

// Charger les variables d'environnement
dotenv.config();

// Configuration
const DESCRIPTIONS_DIR = path.join(process.cwd(), 'workflow-descriptions');
const BATCH_SIZE = 100; // Embeddings par batch
const UPSERT_BATCH_SIZE = 50; // Vecteurs √† upserter en une fois

// Initialiser les services
const pinecone = new Pinecone({
  apiKey: process.env.PINECONE_API_KEY
});

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * G√©n√©rer des embeddings pour un batch de descriptions
 */
async function generateEmbeddings(descriptions) {
  try {
    console.log(`üß† G√©n√©ration embeddings pour ${descriptions.length} descriptions...`);
    
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: descriptions,
      encoding_format: 'float'
    });

    return response.data.map(item => item.embedding);
  } catch (error) {
    console.error('‚ùå Erreur g√©n√©ration embeddings:', error.message);
    throw error;
  }
}

/**
 * Pr√©parer les vecteurs pour Pinecone
 */
function prepareVectors(workflowData, embeddings) {
  return workflowData.map((workflow, index) => {
    // Utiliser le nom du fichier comme ID (sans .json)
    const id = workflow.filename.replace('.json', '');
    
    return {
      id: id,
      values: embeddings[index],
      metadata: {
        filename: workflow.filename,
        name: workflow.workflowName,
        description: workflow.description,
        nodeCount: workflow.metadata?.nodeCount || 0,
        nodeTypes: workflow.metadata?.nodeTypes || [],
        analysisDate: workflow.metadata?.analysisDate,
        model: workflow.metadata?.model || 'gpt-4o',
        // Ajouter les premiers mots de la description pour la recherche rapide
        descriptionSnippet: workflow.description?.substring(0, 500) || '',
        // Indexer aussi les types de n≈ìuds pour la recherche
        nodesString: (workflow.metadata?.nodeTypes || []).join(', ')
      }
    };
  });
}

/**
 * Upserter les vecteurs dans Pinecone
 */
async function upsertVectors(index, vectors) {
  try {
    console.log(`üì§ Upload de ${vectors.length} vecteurs vers Pinecone...`);
    
    // Upserter par batches
    for (let i = 0; i < vectors.length; i += UPSERT_BATCH_SIZE) {
      const batch = vectors.slice(i, i + UPSERT_BATCH_SIZE);
      
      await index.upsert(batch);
      
      console.log(`  ‚úÖ Batch ${Math.floor(i / UPSERT_BATCH_SIZE) + 1}/${Math.ceil(vectors.length / UPSERT_BATCH_SIZE)} upload√©`);
      
      // Petite pause pour √©viter le rate limiting
      if (i + UPSERT_BATCH_SIZE < vectors.length) {
        await new Promise(resolve => setTimeout(resolve, 500));
      }
    }
    
    console.log(`üéâ ${vectors.length} vecteurs upload√©s avec succ√®s`);
    
  } catch (error) {
    console.error('‚ùå Erreur upload Pinecone:', error.message);
    throw error;
  }
}

/**
 * Script principal de r√©indexation
 */
async function main() {
  console.log('üöÄ R√©indexation Pinecone avec descriptions GPT-4');
  
  try {
    // V√©rifier les variables d'environnement
    if (!process.env.PINECONE_API_KEY || !process.env.OPENAI_API_KEY) {
      throw new Error('Variables d\'environnement manquantes: PINECONE_API_KEY et/ou OPENAI_API_KEY');
    }
    
    // Trouver le fichier de descriptions le plus r√©cent
    console.log(`üìÅ Lecture du dossier: ${DESCRIPTIONS_DIR}`);
    const files = await fs.readdir(DESCRIPTIONS_DIR);
    const descriptionFiles = files.filter(f => f.startsWith('workflow-descriptions-') && f.endsWith('.json'));
    
    if (descriptionFiles.length === 0) {
      throw new Error('Aucun fichier de descriptions trouv√©. Ex√©cutez d\'abord generate-workflow-descriptions.js');
    }
    
    // Prendre le plus r√©cent
    const latestFile = descriptionFiles.sort().reverse()[0];
    const descriptionsPath = path.join(DESCRIPTIONS_DIR, latestFile);
    
    console.log(`üìÑ Utilisation du fichier: ${latestFile}`);
    
    // Charger les descriptions
    const descriptionsData = JSON.parse(await fs.readFile(descriptionsPath, 'utf-8'));
    const workflows = descriptionsData.workflows.filter(w => w.description); // Seuls les workflows avec descriptions
    
    console.log(`üìä ${workflows.length} workflows avec descriptions trouv√©s`);
    
    if (workflows.length === 0) {
      throw new Error('Aucun workflow avec description valide trouv√©');
    }
    
    // Connecter √† Pinecone
    const indexName = process.env.PINECONE_WORKFLOW_INDEX || 'n8n-workflows';
    console.log(`üîå Connexion √† l'index Pinecone: ${indexName}`);
    
    const index = pinecone.index(indexName);
    
    // V√©rifier les stats de l'index actuel
    try {
      const stats = await index.describeIndexStats();
      console.log(`üìä Index actuel: ${stats.totalVectorCount} vecteurs`);
      
      // Optionnel: vider l'index avant r√©indexation
      console.log('üóëÔ∏è  Note: Vous pouvez vider l\'index existant si n√©cessaire');
    } catch (error) {
      console.log('‚ö†Ô∏è  Impossible de r√©cup√©rer les stats de l\'index (normal si nouveau)');
    }
    
    // Traiter par batches pour les embeddings
    const allVectors = [];
    
    for (let i = 0; i < workflows.length; i += BATCH_SIZE) {
      const batch = workflows.slice(i, i + BATCH_SIZE);
      const batchIndex = Math.floor(i / BATCH_SIZE) + 1;
      const totalBatches = Math.ceil(workflows.length / BATCH_SIZE);
      
      console.log(`\nüì¶ Batch ${batchIndex}/${totalBatches} - ${batch.length} workflows`);
      
      // Extraire les descriptions pour les embeddings
      const descriptions = batch.map(w => {
        // Combiner le nom et la description pour un embedding plus riche
        return `${w.workflowName}\n\n${w.description}`;
      });
      
      // G√©n√©rer les embeddings
      const embeddings = await generateEmbeddings(descriptions);
      
      // Pr√©parer les vecteurs
      const vectors = prepareVectors(batch, embeddings);
      allVectors.push(...vectors);
      
      console.log(`‚úÖ Batch ${batchIndex} trait√©`);
      
      // Pause entre les batches
      if (i + BATCH_SIZE < workflows.length) {
        console.log('‚è±Ô∏è  Pause 1s...');
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }
    
    console.log(`\nüéØ ${allVectors.length} vecteurs pr√©par√©s, upload vers Pinecone...`);
    
    // Upserter tous les vecteurs
    await upsertVectors(index, allVectors);
    
    // V√©rifier les nouvelles stats
    console.log('\n‚è±Ô∏è  Attente de la synchronisation Pinecone...');
    await new Promise(resolve => setTimeout(resolve, 5000));
    
    try {
      const newStats = await index.describeIndexStats();
      console.log(`üìä Nouvel index: ${newStats.totalVectorCount} vecteurs`);
    } catch (error) {
      console.log('‚ö†Ô∏è  Impossible de v√©rifier les nouvelles stats');
    }
    
    // Sauvegarder un rapport de r√©indexation
    const reportPath = path.join(DESCRIPTIONS_DIR, `reindex-report-${new Date().toISOString().split('T')[0]}.json`);
    const report = {
      reindexDate: new Date().toISOString(),
      sourceFile: latestFile,
      totalWorkflows: workflows.length,
      successfulVectors: allVectors.length,
      indexName: indexName,
      model: 'text-embedding-3-small',
      batchSize: BATCH_SIZE,
      upsertBatchSize: UPSERT_BATCH_SIZE
    };
    
    await fs.writeFile(reportPath, JSON.stringify(report, null, 2));
    
    console.log('\nüéâ R√âINDEXATION TERMIN√âE AVEC SUCC√àS !');
    console.log(`üìä ${allVectors.length} workflows index√©s avec leurs descriptions GPT-4`);
    console.log(`üìù Rapport sauvegard√©: ${reportPath}`);
    console.log('\nüí° Votre syst√®me RAG va maintenant √™tre beaucoup plus pr√©cis !');
    
  } catch (error) {
    console.error('‚ùå Erreur principale:', error);
    process.exit(1);
  }
}

// Test de recherche apr√®s r√©indexation
async function testSearch(query = "workflow qui envoie des emails automatiquement") {
  try {
    console.log(`\nüîç Test de recherche: "${query}"`);
    
    // G√©n√©rer embedding pour la requ√™te
    const queryEmbedding = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: query,
      encoding_format: 'float'
    });
    
    // Rechercher dans Pinecone
    const indexName = process.env.PINECONE_WORKFLOW_INDEX || 'n8n-workflows';
    const index = pinecone.index(indexName);
    
    const searchResults = await index.query({
      vector: queryEmbedding.data[0].embedding,
      topK: 5,
      includeMetadata: true
    });
    
    console.log(`üìã ${searchResults.matches?.length || 0} r√©sultats trouv√©s:`);
    
    if (searchResults.matches) {
      searchResults.matches.forEach((match, i) => {
        console.log(`\n${i + 1}. ${match.metadata?.name} (score: ${match.score.toFixed(3)})`);
        console.log(`   üìÅ ${match.metadata?.filename}`);
        console.log(`   üìù ${match.metadata?.descriptionSnippet?.substring(0, 150)}...`);
      });
    }
    
  } catch (error) {
    console.error('‚ùå Erreur test recherche:', error.message);
  }
}

// Ex√©cuter si appel√© directement
if (import.meta.url === `file://${process.argv[1]}`) {
  main().then(() => {
    // Test optionnel apr√®s r√©indexation
    if (process.argv.includes('--test')) {
      return testSearch();
    }
  });
}

export { main, testSearch }; 