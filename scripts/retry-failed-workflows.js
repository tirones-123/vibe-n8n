import OpenAI from 'openai';
import fs from 'fs/promises';
import path from 'path';
import dotenv from 'dotenv';

// Charger les variables d'environnement
dotenv.config();

// Configuration
const WORKFLOWS_DIR = path.join(process.cwd(), 'workflows-rag-optimized');
const DESCRIPTIONS_DIR = path.join(process.cwd(), 'workflow-descriptions');
const BATCH_SIZE = 5; // Traiter 5 workflows en parall√®le
const DELAY_BETWEEN_BATCHES = 2000; // 2 secondes entre les batches

// Initialiser OpenAI
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Prompt syst√®me pour GPT-4 (m√™me que dans le script principal)
const ANALYSIS_PROMPT = `You are an n8n automation expert. Analyze this workflow JSON and generate a detailed and precise description in English.

The description must be structured as follows:

**Main Objective:** [In one sentence, what does this workflow do]

**Trigger:** [How the workflow starts - webhook, cron, manual, etc.]

**Detailed Flow:**
1. [Step 1 with involved node]
2. [Step 2 with involved node]
3. [...]

**Services/APIs Used:** [List of external services - Slack, Gmail, etc.]

**Use Cases:** [In what contexts would this workflow be useful]

**Key Features:** [Transformations, conditions, loops, etc.]

Be precise about the actual actions performed and data manipulated. Use natural language and avoid excessive technical jargon.`;

/**
 * Analyser un workflow avec GPT-4 (m√™me fonction que dans le script principal)
 */
async function analyzeWorkflow(workflowData, filename) {
  try {
    console.log(`üîç R√©-analyse de ${filename}...`);
    
    // Extraire les informations cl√©s du workflow
    const nodeTypes = workflowData.nodes?.map(n => n.type) || [];
    const nodeNames = workflowData.nodes?.map(n => n.name) || [];
    const workflowName = workflowData.name || filename.replace('.json', '');
    
    // Cr√©er un r√©sum√© structur√© pour GPT-4
    const workflowSummary = {
      name: workflowName,
      nodeCount: workflowData.nodes?.length || 0,
      nodeTypes: [...new Set(nodeTypes)],
      nodeNames: nodeNames,
      connections: Object.keys(workflowData.connections || {}),
      settings: workflowData.settings || {}
    };
    
    const prompt = `Workflow JSON to analyze:
Name: ${workflowName}
Number of nodes: ${workflowSummary.nodeCount}
Node types: ${workflowSummary.nodeTypes.join(', ')}

Complete JSON:
${JSON.stringify(workflowData, null, 2)}

Generate a detailed description following the requested format.`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4o', // Using GPT-4 Omni for better analysis
      messages: [
        { role: 'system', content: ANALYSIS_PROMPT },
        { role: 'user', content: prompt }
      ],
      temperature: 0.1, // Low for consistent descriptions
      max_tokens: 2000
    });

    const description = response.choices[0]?.message?.content;
    
    if (!description) {
      throw new Error('No description generated');
    }

    console.log(`‚úÖ ${filename} r√©-analys√© avec succ√®s`);
    
    return {
      filename,
      workflowName,
      description,
      metadata: {
        nodeCount: workflowSummary.nodeCount,
        nodeTypes: workflowSummary.nodeTypes,
        analysisDate: new Date().toISOString(),
        model: 'gpt-4o',
        retry: true
      }
    };

  } catch (error) {
    console.error(`‚ùå Erreur r√©-analyse ${filename}:`, error.message);
    return {
      filename,
      workflowName: workflowData.name || filename,
      description: null,
      error: error.message,
      retry: true
    };
  }
}

/**
 * Identifier les workflows qui ont √©chou√© avec "Connection error"
 */
async function identifyFailedWorkflows() {
  try {
    // Trouver le fichier de descriptions le plus r√©cent
    const files = await fs.readdir(DESCRIPTIONS_DIR);
    const descriptionFiles = files.filter(f => f.startsWith('workflow-descriptions-') && f.endsWith('.json'));
    
    if (descriptionFiles.length === 0) {
      throw new Error('Aucun fichier de descriptions trouv√©');
    }
    
    const latestFile = descriptionFiles.sort().reverse()[0];
    const descriptionsPath = path.join(DESCRIPTIONS_DIR, latestFile);
    
    console.log(`üìÑ Lecture du fichier: ${latestFile}`);
    
    // Charger les descriptions
    const descriptionsData = JSON.parse(await fs.readFile(descriptionsPath, 'utf-8'));
    
    // Identifier les workflows avec erreurs de connexion
    const failedWorkflows = descriptionsData.workflows.filter(w => 
      w.error && (
        w.error.includes('Connection error') ||
        w.error.includes('connection error') ||
        w.error.includes('Network error') ||
        w.error.includes('network error') ||
        w.error.includes('ENOTFOUND') ||
        w.error.includes('ECONNRESET') ||
        w.error.includes('timeout')
      )
    );
    
    console.log(`üìä Workflows √©chou√©s trouv√©s: ${failedWorkflows.length}`);
    failedWorkflows.forEach((w, i) => {
      console.log(`  ${i + 1}. ${w.filename} - Erreur: ${w.error}`);
    });
    
    return {
      descriptionsData,
      failedWorkflows,
      descriptionsPath
    };
    
  } catch (error) {
    console.error('‚ùå Erreur lecture fichier descriptions:', error.message);
    throw error;
  }
}

/**
 * Relancer l'analyse des workflows √©chou√©s
 */
async function retryFailedWorkflows() {
  console.log('üîÑ Identification et reprise des workflows √©chou√©s');
  
  try {
    // Identifier les workflows √©chou√©s
    const { descriptionsData, failedWorkflows, descriptionsPath } = await identifyFailedWorkflows();
    
    if (failedWorkflows.length === 0) {
      console.log('üéâ Aucun workflow √©chou√© √† relancer !');
      return;
    }
    
    console.log(`üéØ ${failedWorkflows.length} workflows √† relancer`);
    
    // Charger les contenus des workflows √©chou√©s
    const workflowsToRetry = [];
    
    for (const failedWorkflow of failedWorkflows) {
      try {
        const filePath = path.join(WORKFLOWS_DIR, failedWorkflow.filename);
        const content = await fs.readFile(filePath, 'utf-8');
        workflowsToRetry.push({ 
          filename: failedWorkflow.filename, 
          content,
          originalIndex: descriptionsData.workflows.findIndex(w => w.filename === failedWorkflow.filename)
        });
      } catch (error) {
        console.error(`‚ùå Impossible de charger ${failedWorkflow.filename}:`, error.message);
      }
    }
    
    console.log(`üìñ ${workflowsToRetry.length} workflows charg√©s pour reprise`);
    
    // Traiter par batches
    const batches = [];
    for (let i = 0; i < workflowsToRetry.length; i += BATCH_SIZE) {
      batches.push(workflowsToRetry.slice(i, i + BATCH_SIZE));
    }
    
    console.log(`üéØ Traitement en ${batches.length} batches de ${BATCH_SIZE} workflows max`);
    
    let totalSuccess = 0;
    let totalFailed = 0;
    
    for (let i = 0; i < batches.length; i++) {
      console.log(`\nüì¶ Batch ${i + 1}/${batches.length} - Reprise de ${batches[i].length} workflows...`);
      
      const promises = batches[i].map(async ({ filename, content, originalIndex }) => {
        try {
          const workflowData = JSON.parse(content);
          const result = await analyzeWorkflow(workflowData, filename);
          return { ...result, originalIndex };
        } catch (error) {
          console.error(`‚ùå Erreur parsing ${filename}:`, error.message);
          return {
            filename,
            workflowName: filename,
            description: null,
            error: `Parsing error: ${error.message}`,
            originalIndex
          };
        }
      });

      const results = await Promise.all(promises);
      
      // Mettre √† jour le fichier de descriptions avec les nouveaux r√©sultats
      for (const result of results) {
        if (result.originalIndex >= 0) {
          descriptionsData.workflows[result.originalIndex] = result;
        }
      }
      
      // Sauvegarder imm√©diatement
      await fs.writeFile(descriptionsPath, JSON.stringify(descriptionsData, null, 2));
      
      const batchSuccess = results.filter(r => r.description).length;
      const batchFailed = results.filter(r => !r.description).length;
      
      totalSuccess += batchSuccess;
      totalFailed += batchFailed;
      
      console.log(`‚úÖ Batch ${i + 1} termin√©: ${batchSuccess} succ√®s, ${batchFailed} √©checs`);
      console.log(`üíæ Fichier mis √† jour avec les nouvelles analyses`);
      
      // D√©lai entre les batches
      if (i < batches.length - 1) {
        console.log(`‚è±Ô∏è  Pause de ${DELAY_BETWEEN_BATCHES}ms...`);
        await new Promise(resolve => setTimeout(resolve, DELAY_BETWEEN_BATCHES));
      }
    }
    
    // Mettre √† jour les statistiques globales
    descriptionsData.successful = descriptionsData.workflows.filter(w => w.description).length;
    descriptionsData.failed = descriptionsData.workflows.filter(w => !w.description).length;
    descriptionsData.lastRetryDate = new Date().toISOString();
    
    // Sauvegarder le fichier final
    await fs.writeFile(descriptionsPath, JSON.stringify(descriptionsData, null, 2));
    
    console.log('\nüìä REPRISE TERMIN√âE:');
    console.log(`‚úÖ Workflows relanc√©s avec succ√®s: ${totalSuccess}`);
    console.log(`‚ùå Workflows encore en √©chec: ${totalFailed}`);
    console.log(`üìà Total succ√®s dans le fichier: ${descriptionsData.successful}`);
    console.log(`üìâ Total √©checs dans le fichier: ${descriptionsData.failed}`);
    console.log(`üìù Fichier mis √† jour: ${descriptionsPath}`);
    
    if (totalFailed > 0) {
      console.log('\n‚ö†Ô∏è  Quelques workflows ont encore √©chou√©. V√©rifiez les erreurs et relancez si n√©cessaire.');
    } else {
      console.log('\nüéâ Tous les workflows ont √©t√© analys√©s avec succ√®s !');
      console.log('‚úÖ Vous pouvez maintenant continuer avec le script principal si n√©cessaire.');
    }
    
  } catch (error) {
    console.error('‚ùå Erreur principale:', error);
    throw error;
  }
}

// Ex√©cuter si appel√© directement
if (import.meta.url === `file://${process.argv[1]}`) {
  retryFailedWorkflows()
    .then(() => {
      console.log('\nüéØ Reprise termin√©e avec succ√®s !');
      process.exit(0);
    })
    .catch(error => {
      console.error('\n‚ùå Erreur fatale:', error.message);
      process.exit(1);
    });
}

export { retryFailedWorkflows, identifyFailedWorkflows }; 