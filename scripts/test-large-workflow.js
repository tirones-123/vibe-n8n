#!/usr/bin/env node

/**
 * Script de test pour les gros workflows
 * Simule la g√©n√©ration et transmission de workflows volumineux
 */

import fetch from 'node-fetch';
import fs from 'fs/promises';
import path from 'path';

const CONFIG = {
  API_URL: process.env.API_URL || 'http://localhost:3000/api/claude',
  API_KEY: process.env.BACKEND_API_KEY || 'your-api-key',
  TIMEOUT: 900000 // 15 minutes
};

// Tests pour diff√©rentes tailles de workflows
const TEST_CASES = [
  {
    name: 'Small Workflow',
    prompt: 'Cr√©er un simple webhook qui envoie un email',
    expectedSize: '<10KB'
  },
  {
    name: 'Medium Workflow',
    prompt: 'Cr√©er un workflow complexe qui synchronise Slack avec Notion, traite les messages avec OpenAI, sauvegarde dans Airtable, et envoie des notifications Discord',
    expectedSize: '10-50KB'
  },
  {
    name: 'Large Workflow',
    prompt: 'Cr√©er un syst√®me complet de gestion de leads : webhook ‚Üí validation ‚Üí enrichissement Clearbit ‚Üí scoring IA ‚Üí CRM ‚Üí email automation ‚Üí reporting ‚Üí Slack notifications ‚Üí backup S3 ‚Üí monitoring ‚Üí analytics',
    expectedSize: '>50KB'
  },
  {
    name: 'Huge Workflow',
    prompt: 'Cr√©er une plateforme e-commerce compl√®te : gestion commandes ‚Üí inventaire ‚Üí paiements ‚Üí shipping ‚Üí emails ‚Üí analytics ‚Üí reporting ‚Üí BI ‚Üí CRM ‚Üí support ‚Üí notifications ‚Üí backups ‚Üí monitoring ‚Üí compliance ‚Üí multi-tenant ‚Üí API gateway ‚Üí microservices orchestration',
    expectedSize: '>100KB'
  }
];

async function testWorkflowGeneration(testCase) {
  console.log(`\nüß™ === TEST: ${testCase.name} ===`);
  console.log(`üìù Prompt: ${testCase.prompt.substring(0, 100)}...`);
  console.log(`üìè Taille attendue: ${testCase.expectedSize}`);
  
  const startTime = Date.now();
  let chunks = [];
  let compressionUsed = false;
  let chunkingUsed = false;
  let errors = [];

  try {
    console.log('üöÄ Envoi de la requ√™te...');
    
    const response = await fetch(CONFIG.API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${CONFIG.API_KEY}`
      },
      body: JSON.stringify({
        prompt: testCase.prompt
      }),
      timeout: CONFIG.TIMEOUT
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    console.log('üì° Lecture du stream SSE...');
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';
    let workflow = null;
    let explanation = null;

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          try {
            const event = JSON.parse(line.slice(6));
            console.log(`üì® Event: ${event.type}`);
            
            switch (event.type) {
              case 'compression':
                compressionUsed = true;
                console.log(`üóúÔ∏è Compression activ√©e pour ${event.data.nodesCount} n≈ìuds`);
                break;
                
              case 'chunking_start':
                chunkingUsed = true;
                console.log(`üì¶ Chunking activ√©: ${event.data.totalChunks} parties`);
                break;
                
              case 'chunk':
                chunks.push({
                  index: event.data.index,
                  size: event.data.data.length
                });
                console.log(`üì¶ Chunk ${event.data.index + 1}/${event.data.total}: ${event.data.data.length} chars`);
                break;
                
              case 'compressed_complete':
                console.log(`‚úÖ Workflow compress√© re√ßu`);
                workflow = { compressed: true, size: event.data.data.length };
                break;
                
              case 'complete':
                console.log(`‚úÖ Workflow direct re√ßu`);
                workflow = event.data.workflow;
                explanation = event.data.explanation;
                break;
                
              case 'error':
                errors.push(event.data.error);
                console.error(`‚ùå Erreur: ${event.data.error}`);
                break;
            }
          } catch (e) {
            console.warn(`‚ö†Ô∏è Parse error: ${e.message}`);
          }
        }
      }
    }

    const duration = Date.now() - startTime;
    
    // Analyse des r√©sultats
    let workflowSize = 0;
    if (workflow) {
      if (workflow.compressed) {
        workflowSize = workflow.size;
      } else {
        const jsonString = JSON.stringify(workflow);
        workflowSize = Buffer.byteLength(jsonString, 'utf8');
      }
    }

    console.log(`\nüìä === R√âSULTATS ${testCase.name} ===`);
    console.log(`‚è±Ô∏è Dur√©e: ${(duration / 1000).toFixed(1)}s`);
    console.log(`üìè Taille workflow: ${(workflowSize / 1024).toFixed(1)}KB`);
    console.log(`üóúÔ∏è Compression utilis√©e: ${compressionUsed ? '‚úÖ' : '‚ùå'}`);
    console.log(`üì¶ Chunking utilis√©: ${chunkingUsed ? '‚úÖ' : '‚ùå'}`);
    console.log(`üì¶ Nombre de chunks: ${chunks.length}`);
    console.log(`‚ùå Erreurs: ${errors.length}`);
    
    if (workflow && !workflow.compressed) {
      console.log(`üìä N≈ìuds g√©n√©r√©s: ${workflow.nodes?.length || 0}`);
      console.log(`üîó Connexions: ${Object.keys(workflow.connections || {}).length}`);
    }

    // Sauvegarder pour analyse
    const result = {
      testCase: testCase.name,
      duration,
      workflowSize,
      compressionUsed,
      chunkingUsed,
      chunksCount: chunks.length,
      chunks,
      errorsCount: errors.length,
      errors,
      workflow: workflow && !workflow.compressed ? workflow : null,
      explanation,
      timestamp: new Date().toISOString()
    };

    await fs.writeFile(
      path.join(process.cwd(), 'debug', `test-${testCase.name.toLowerCase().replace(/\s+/g, '-')}.json`),
      JSON.stringify(result, null, 2)
    );

    return result;

  } catch (error) {
    console.error(`‚ùå Erreur test ${testCase.name}:`, error.message);
    return {
      testCase: testCase.name,
      error: error.message,
      duration: Date.now() - startTime,
      timestamp: new Date().toISOString()
    };
  }
}

async function runAllTests() {
  console.log('üß™ === TESTS DE GROS WORKFLOWS ===');
  console.log(`üåê API URL: ${CONFIG.API_URL}`);
  console.log(`‚è∞ Timeout: ${CONFIG.TIMEOUT}ms`);
  
  const results = [];
  
  // Cr√©er le dossier debug si n√©cessaire
  try {
    await fs.mkdir(path.join(process.cwd(), 'debug'), { recursive: true });
  } catch (e) {}

  for (const testCase of TEST_CASES) {
    const result = await testWorkflowGeneration(testCase);
    results.push(result);
    
    // Pause entre les tests
    console.log('‚è≥ Pause de 5 secondes...');
    await new Promise(resolve => setTimeout(resolve, 5000));
  }

  // R√©sum√© final
  console.log('\nüìä === R√âSUM√â FINAL ===');
  const summary = {
    totalTests: results.length,
    successful: results.filter(r => !r.error).length,
    failed: results.filter(r => r.error).length,
    compressionUsed: results.filter(r => r.compressionUsed).length,
    chunkingUsed: results.filter(r => r.chunkingUsed).length,
    averageDuration: results.reduce((sum, r) => sum + r.duration, 0) / results.length,
    totalErrors: results.reduce((sum, r) => sum + (r.errorsCount || 0), 0),
    results
  };

  console.log(`‚úÖ Tests r√©ussis: ${summary.successful}/${summary.totalTests}`);
  console.log(`üóúÔ∏è Compression utilis√©e: ${summary.compressionUsed} fois`);
  console.log(`üì¶ Chunking utilis√©: ${summary.chunkingUsed} fois`);
  console.log(`‚è±Ô∏è Dur√©e moyenne: ${(summary.averageDuration / 1000).toFixed(1)}s`);
  console.log(`‚ùå Total erreurs: ${summary.totalErrors}`);

  // Sauvegarder le r√©sum√©
  await fs.writeFile(
    path.join(process.cwd(), 'debug', 'test-summary.json'),
    JSON.stringify(summary, null, 2)
  );

  console.log('\nüíæ R√©sultats sauvegard√©s dans debug/');
}

// Ex√©cuter si appel√© directement
if (import.meta.url === `file://${process.argv[1]}`) {
  runAllTests().catch(console.error);
} 